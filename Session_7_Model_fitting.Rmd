---
title: "Modelling workshop: Session 7"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r klippy, echo=FALSE, include=TRUE}
library(klippy)
klippy::klippy('')
```


## Session 7 Model fitting

**Session 7** contains an introduction to fitting models to data using **Maximum Likelihood** and **Bayesian** approaches.

### 7a - Introduction to model fitting

Models of all sorts can and are fit to data, including the statistical and mathematical models we have been introduced to here, but also **phylogenetic** methods used to analyse gene or genomic sequence data.

Some mechanistic mathematical models are run without fitting the models to data, as we have seen, but often an informative way to understand a system is to fit models to the data, potentially in an iterative way, where models are developed, data collected based on the model parameters, the model fit to the data, which generates new questions, etc. in an iterative fashion until the system is well understood.

Earlier we were introduced to linear regression in **Session 4** and more again in **Session 6**. However, what was perhaps not clear is that every time we ran a model it was actually being **fit** to data. We started to explore that at the end of **Session 6**.

This means that we took the model with the factors that we hypothesized were predictors of the data and fit the model to these data. We made different **assumptions** about the data and in our models. For example, we used linear models first that assumed that the data had **linear relationships**, **multivariate normality**, **no or little multicollinearity**, **no auto-correlation** and there was **homoscedasticity**.

We will not go into all of these here due to time, but will bring these concepts in below when needed.

But these assumptions were made (knowingly or not) and then the model was fit to estimate the parameters such as the intercept $\beta_0$ where $x$ is 0 and the slope of the line, $\beta_1$ that shows the relationship between $x$ and $y$ in the below:

$$
y = \beta_0 + \beta_1 x + \epsilon
$$

The method used to fit these models, but mostly not thought about becuase it is inbuilt and we use it so often, is a type of **Generalized Least Squares Estimation** (or **GLS**) method called **Ordinary Least Squares** (or **OLS**). OLS makes two crucial assumptions:

1) The variance of the errors of the regression model is constant i.e., the errors are **homoskedastic**, and
2) The errors are **not correlated** with each other or with themselves.

However, GLS is suitable for fitting linear models on data sets that exhibit heteroskedasticity (i.e., non-constant variance) and/or auto-correlation, which real world data sets often have.

Least squares fitting was also used for the **logistic regression** we used to analyse the serology data in **Session 6**.

However, there are other approaches to fitting models to data. These are typically presented as **Maximum likelihood** and **Bayesian** methods. Many phylogenetic tools use these methods, such as in this list here:  https://en.wikipedia.org/wiki/List_of_phylogenetics_software.

However, interestingly (or not!) maximum likelihood is really a special case of a Bayesian model and GLS a special case of maximum likelihood, like OLS is a special case of GLS. So, they are all related *but* they all make **different assumptions**.

For example, for the linear model fitting, if the errors belong to a normal distribution the least squares estimators are also the maximum likelihood estimators. The **nls()** function (which stands for 'Nonlinear Least Squares') can be used to find parameter values for relatively simple *non-linear* functions.

### 7b - Maximum Likelihood approaches

Here, we will use the **bbmle** package. However, there are several packages and methods for estimating model parameters using maximum likelihood.

**Likelihood** is a fundamental component of statistical inference. The likelihood function, often denoted as **L(θ|X)**, describes the *probability of observing the data X given a specific set of parameters θ*. It quantifies the plausibility of different parameter values, given the observed data.

The **bbmle** package allows us to fit a lot of models that do not assume a *normal* likelihood, so Poisson, binomial, multinomial, beta, gamma, and other distributions. 

**bbmle** is also able to fit data to **dynamic models** where the value of the **state variables** like the **susceptible, infected and recovered** in **SIR models**, change through time. When this happens, the distributions are not likely to be normal.

#### Example 1

The first worked example here comes from some class notes Prof Tom Hobbs of Colorado State University, USA, used in 2011. Let's start with the code then work through it.

Here's the code:  

```{r, class.source='klippy'}
#estimating parameters in observation model  
# get bbmle library  
  
library(bbmle)  
  
#Generates random counts for a calibration curve  
a = -2.25  
b = .60  
  
#The independent variable available to all functions.  
x = sort(runif(50,1,20))  
  
#function for model  
yhat = function(a,b){  
 	mu = a + b*x  
 	mu[mu<0]=0  
 	return(mu)  
 	}  
  
# The generating function  
y = yhat(a=a,b=b)  
plot(x,y, typ='l', col="red", cex.lab = 1.5, cex = 1.5,  xlab="Number Present", ylab="Number Observed",  
ylim=c(0,12))  
#generate data from function  
data=rpois(length(y),y)  
points(x,data)  
  
#The function for the negative log likelihood  
LL1 = function (a,b,y=data){  
 	mu = yhat(a=a,b=b)  
 	#get the total negative log likelihood  
 	loglike= -sum(dpois(y,mu,log=TRUE))  
 	return(loglike)  
}  
  
#The call to bbmle  
m1 = mle2(minuslogl = LL1, start = list(a = -2, b = .5),  control = list(maxit = 5000))  

#Analysis  
summary(m1)  

#make a vector for plotting the esimates  
y.est=numeric(length(y))  
#get the estimated parameters  
a.est = coef(m1)[1]  
b.est = coef(m1)[2]  
y.est=yhat(a=a.est,b=b.est)  
  
#plot the fitted model against the generating model and the  data  
lines(x,y.est, lty='dashed')  
legend(3,12,c("generating","estimated"), lty=c("solid",  
"dashed"), bty="n")  
```

Here's how we can break that down. 

First, the likelihood estimation can be considered to be four parts:
* a function describing a model
* a function that calculates the negative log-likelihood
* a call to a function that does the non-linear search for the best parameter values, and
* analysis of the results.

##### The model 

Here is the model:

```{r, class.source='klippy'}
#function for model  
yhat = function(a,b){  
 	mu = a + b*x  
 	mu[mu<0]=0  
 	return(mu)  
 	}  
```

Here we define the parameters and independent variable for the model and simulate the data:

```{r, class.source='klippy'}
# parameters
a = -2.25  
b = .60  
# independent variable available to all functions.  
x = sort(runif(50,1,20))  


y = yhat(a=a,b=b)  
data=rpois(length(y),y)  
```

The function **yhat()** represents the deterministic part of the model and **rpois()** adds stochasticity (noise, or randomness) using a Poisson distribution.

##### The negative log-likelihood  
The next step is to use the model function and the data to create function that returns the  summed negative log-likelihood.  This is the part of the code that does that:  

```{r, class.source='klippy'}
#The function for the negative log likelihood  
LL1 = function (a,b,y=data){  
 	mu = yhat(a=a,b=b)  
 	#get the total negative log likelihood  
 	loglike= -sum(dpois(y,mu,log=TRUE))  
 	return(loglike)  
}  
```

##### Call mle2

Next, we use the **mle2()** function to obtain the maximum likelihood estimates.
  
```{r, class.source='klippy'}
##The call to bbmle  
#m1 = mle2(minuslogl = LL1, start = list(a = -2, b = .8),  
#control = list(maxit = 5000))  
```

The rest of the code simply plots what we are doing.

#### Example 2

This example is taken directly from another class, this time from James Holland Jones from 2018: http://web.stanford.edu/class/earthsys214/notes/fit.html

Here we will fit an **SIR** model to data. 

First, we define the epidemic model.

```{r, class.source='klippy'}
require(deSolve)
sir <- function(t,x,parms){
    S <- x[1]
    I <- x[2]
    R <- x[3]
  with(as.list(parms),
{
    dS <- -beta*S*I
    dI <- beta*S*I - nu*I
    dR <- nu*I
    res <- c(dS,dI,dR)
  list(res)
})
}
```

Next, to solve the system of equations, we need to use the **ode()** function (itself a wrapper another function **lsoda()**). 

For **ode()** to work we need to provide an initial *vector* of the *state variables*, the *times* over which integration takes place, the *function* that defines the ODEs, and a vector of *parameters* for the ODEs.

In this example the data are *deaths* in *Mumbai* in India from a disease outbreak.

```{r, class.source='klippy'}
N <- 1e4
parms <- c(N=N,beta=0.0001, nu = 1/7)
times <- seq(0,30,0.1)
x0 <- c(N,1,0)
stateMatrix <- ode(y=x0, times, sir, parms)
colnames(stateMatrix) <- c("time","S","I","R")
plot(stateMatrix[,"time"], stateMatrix[,"S"], type="l", lwd=2, 
     xlab="Time", ylab="Population Size")
lines(stateMatrix[,"time"], stateMatrix[,"I"], col="red", lwd=2)
lines(stateMatrix[,"time"], stateMatrix[,"R"], col="green", lwd=2)
legend("right", c("S","I","R"), col=c("black","red","green"), lwd=2)
mumbai <- c(0, 4, 10, 15, 18, 21, 31, 51, 53, 97, 125, 183, 292, 390, 448,
            641, 771, 701, 696, 867, 925, 801, 580, 409, 351, 210, 113, 65, 
            52, 51, 39, 33)
cummumbai <- cumsum(mumbai)
weeks <- 0:31
plot(weeks, cummumbai, pch=16, xlab="Weeks", ylab="Cumulative Deaths")
```

Next we write the model likelihood. 

We need a *likelihood* function that takes the same arguments as the first model we wrote.

Note that there are some differences here as we use a logit-transformation **qlogis()** to ensure that $β$ and $ν$ are probabilities. 

We pass the optimization function logit-transformed values which the function then back-transforms using **plogis()**. Similarly, for the population size and number of initial infections, we pass log-transformed values which the function back-transforms.

See if you can find these terms in this code.

Note that this can take a long time. If you make *trace=2* within the *control* argument above it will provide feedback at the command line about the progress of the optimization. 

Now let's run the code:

```{r, class.source='klippy'}
require(bbmle)
# likelihood function
sirLL <- function(lbeta, lnu, logN, logI0) {
    parms <- c(beta=plogis(lbeta), nu=plogis(lnu))
    x0 <- c(S=exp(logN), I=exp(logI0), R=0)
    out <- ode(y=x0, weeks, sir, parms)
    SD <- sqrt(sum( (cummumbai-out[,4])^2)/length(weeks) )
    -sum(dnorm(cummumbai, mean=out[,4], sd=SD, log=TRUE))
}
# minimize negative-log-likelihood

fit <- mle2(sirLL, 
            start=list(lbeta=qlogis(1e-5), 
                lnu=qlogis(.2), 
                logN=log(1e6), logI0=log(1) ),  
            method="Nelder-Mead",
            control=list(maxit=1E5,trace=0),
            trace=FALSE)

summary(fit)
theta <- as.numeric(c(plogis(coef(fit)[1:2]),
                  exp(coef(fit)[3:4])) )
```

Next let's plot this:

```{r, class.source='klippy'}
parms <- c(beta=theta[1], nu = theta[2])
times <- seq(0,30,0.1)
x0 <- c(theta[3],theta[4],0)
stateMatrix1 <- ode(y=x0, times, sir, parms)
colnames(stateMatrix1) <- c("time","S","I","R")
plot(stateMatrix1[,"time"], stateMatrix1[,"R"], type="l", lwd=2, 
     xaxs="i", xlab="Time", ylab="Cumulative Deaths")
points(weeks, cummumbai, pch=16, col="red")
```

However, the outcomes (deaths) can be modelled as a **Poisson** random variable. Let's see if that does any better by using this code:

```{r, class.source='klippy'}
sirLL2 <- function(lbeta, lnu, logN, logI0) {
    parms <- c(beta=plogis(lbeta), nu=plogis(lnu))
    x0 <- c(S=exp(logN), I=exp(logI0), R=0)
    out <- ode(y=x0, weeks, sir, parms)
    -sum(dpois(cummumbai, lambda=out[,4], log=TRUE))
}

fit.pois <- mle2(sirLL2, 
                 start=list(lbeta=qlogis(1e-5), 
                     lnu=qlogis(.2), 
                     logN=log(1e6), logI0=log(1) ),  
                 method="Nelder-Mead",
                 control=list(maxit=1E5,trace=0),
                 trace=FALSE)

summary(fit.pois)
theta2 <- as.numeric(c(plogis(coef(fit.pois)[1:2]),
                  exp(coef(fit.pois)[3:4])) )
```

Note that this does not estimate the standard errors - you can see the **NaN** which means that they have not been estimated.

The estimate for $β$ is similar to that from the normal likelihood model above, but $ν$ is different. The $ν$ removal rate parameter estimate for the Poisson likelihood is seven times lower than that for the normal likelihood.

Let's plot this and see which looks like it fits the data better:
 
```{r, class.source='klippy'}
parms <- c(beta=theta2[1], nu = theta2[2])
times <- seq(0,30,0.1)
x0 <- c(theta2[3],theta2[4],0)
stateMatrix2 <- ode(y=x0, times, sir, parms)
colnames(stateMatrix2) <- c("time","S","I","R")
plot(stateMatrix2[,"time"], stateMatrix2[,"R"], type="l", lwd=2, 
     xaxs="i", xlab="Time", ylab="Cumulative Deaths")
lines(stateMatrix1[,"time"], stateMatrix1[,"R"], col=grey(0.85), lwd=2)
points(weeks, cummumbai, pch=16, col="red")
legend("topleft", c("Poisson", "Gaussian"), lwd=2, col=c("black",grey(0.85)))
```

The fit of the normal, measurement-error model to fit the data better than the Poisson measurement-error model.

### 7c - Bayesian approaches

**Bayesian analyses** are a powerful statistical framework that provide a systematic and flexible approach to inferential reasoning and analyses.

Fundamentally, Bayesian analysis combines prior knowledge or beliefs about a phenomenon (including parameter estimates) with observed data to update and refine our understanding. This is achieved through **Bayes' theorem**.

Bayes' theorem quantifies the **posterior probability** of an event given **prior knowledge** and **observed data**. The prior represents our initial beliefs, while the **likelihood** function describes how the data inform us about the underlying parameters of interest. We met the likelihood above.

There are other benefits of Bayesian analyses that allow for complex and hierarchical structures which can mean it is extremely useful. Bayesian approaches can be employed to estimate parameters, make predictions, and quantify uncertainty in a wide range of models.

However, Bayesian analyses also pose computational challenges due to the need to estimate **high-dimensional integrals**. **Markov Chain Monte Carlo** (**MCMC**) methods and more recent advances in Bayesian computation have significantly improved the feasibility and scalability of Bayesian analyses, making it available to more people.

Bayes theorem can be expressed mathematically as:

$P(θ|X) = (P(X|θ) * P(θ)) / P(X)$

Here, **P(θ|X)** represents the **posterior probability** of the parameters given the data X, **P(X|θ)** is the **likelihood function**, **P(θ)** denotes the prior probability of the parameters, and **P(X)** is the marginal probability of the data.

Bayesian methods are more complex to learn in a short period of time, but there are some places that they can be more easily learned. One is here:
https://www.precision-analytics.ca/articles/a-gentle-inla-tutorial/

Note that some researchers have made Bayesian analyses very accessible, such as the writers of **BEAST** (short for Bayesian Evolutionary Analysis Sampling Trees) for **phylogenetic analyses**, which is here: https://beast.community/.

##### Example of Bayesian analysis using R and JAGS

Here we will introduce Bayesian analyses through using another piece of software and R package, **JAGS** (Just Another Gibbs Sampler). If you have not, you need do download and install JAGS from here: https://sourceforge.net/projects/mcmc-jags/files/

First we load the libraries and code the model.

```{r, class.source='klippy'}
require(coda)
library(rjags)
set.seed(432104)
n <- 1000
x <- rnorm(n, 0, 5)
bayes_model <-"
  model {
    for (i in 1:N){
    x[i] ~ dnorm(mu, tau)
    }
  mu ~ dnorm(0,.0001)
  tau <- pow(sigma, -2)
  sigma ~ dunif(0,100)
}
"
bayes_model.spec<-textConnection(bayes_model)
```

Note that this simple model has three main parts in the **bayes_model** script: our parameter priors, our data model and derived parameters.

* The loop "for (i in 1:N)" indicates that the following statements will be repeated for each value of "i" from 1 to "N".
* "x[i] ~ dnorm(mu, tau)" specifies that the variable "x[i]" follows a normal distribution with mean "mu" and precision (inverse variance) "tau". This is the likelihood term of the model, and "x[i]" is observed data or a response variable.
* "mu ~ dnorm(0, .0001)" specifies a prior distribution for the parameter "mu". It assigns a normal distribution with mean 0 and precision 0.0001 (equivalent to a standard deviation of 100) to "mu". This prior suggests a relatively strong prior belief that "mu" is close to 0.
* "tau <- pow(sigma, -2)" calculates the precision "tau" as the reciprocal of the squared value of "sigma". This relationship ensures that "tau" and "sigma" are inversely related.
* "sigma ~ dunif(0, 100)" specifies a prior distribution for the parameter "sigma". It assigns a uniform distribution between 0 and 100 to "sigma". This prior suggests that "sigma" can take any value within this range with equal probability.

We then create and run the model. We will run it for 4 chains (runs) and allow our model to be run in the adaptive phase.  We won't cover that here.

```{r, class.source='klippy'}
jags <- jags.model(bayes_model.spec,
                   data = list('x' = x,
                               'N' = n),
                   n.chains=4,
                   n.adapt=100)
update(jags, 1000)

```

Next we can randomly sample from the results to generate the posterior samples and plot the results from the model. Here we will look at the $\mu$ parameter:

```{r, class.source='klippy'}
jags1<-jags.samples(jags,
             c('mu', 'tau'),
             1000)
summary(jags1)
plot(jags1[[1]])
plot(density(jags1[[1]]))

```

The *hairy caterpillar* looking plot and *normal* distribution is a good indication that there has been good mixing and convergence of the model. If you get a *snake* looking plot and a *non-normal* dsitribution, this is an indication that there is something wrong. A first usual first step is to run your chains longer, but if that does not fix it, it that it will require more careful thought.

Let's look at the results we have in other ways using different functions. We can save the results as a new data set:

```{r, class.source='klippy'}
samps<-coda.samples(jags, variable.names=c("mu", "tau"), n.iter=1000)
```

Now we can summarize those in different ways including by using the **summary()** function and different plot functions, such as **densplot()** and **traceplot()**:

```{r, class.source='klippy'}
#Numerical summary of each parameter:
summary(samps)
densplot(samps)
traceplot(samps)

library(bayesplot)
mcmc_intervals(samps, pars = c("mu", "tau"))

mcmc_areas(
  samps, 
  pars = c("mu", "tau"),
  prob = 0.8, # 80% intervals
  prob_outer = 0.99, # 99%
  point_est = "mean"
)
```

The plots are not so useful because of the scales, so let's plot them differently:

```{r, class.source='klippy'}
color_scheme_set("green")
mcmc_hist(samps, pars = c("mu", "tau"))

```

Now you can see that these histograms show the same results for the parameter estimates from the summaries of the data using the **summary()** function above.

To view separate histograms of each of the four Markov chains we can use the **mcmc_hist_by_chain()** function, which plots each chain in a separate facet in the plot.

```{r, class.source='klippy'}
mcmc_hist_by_chain(samps, pars = c("mu", "tau"))

```
And now we can plot these in other ways that all show us that the different chains have converged and are giving similar results.

```{r, class.source='klippy'}
mcmc_violin(samps, pars = c("mu", "tau"), probs = c(0.1, 0.5, 0.9))
color_scheme_set("gray")

```

We can plot all the values to see is there are clusters by chain:

```{r, class.source='klippy'}
mcmc_scatter(samps, pars = c("mu", "tau"), 
             size = 1.5, alpha = 0.5)
```

Or overlay the chains on top like earlier:

```{r, class.source='klippy'}
mcmc_trace(samps, pars = c("mu", "tau"))
```

Or plot the credible intervals:

```{r, class.source='klippy'}
mcmc_pairs(samps, pars = c("mu", "tau"),
           off_diag_args = list(size = 1.5))
```

Note that instead of confidence intervals, the variation around Bayesian estimates for parameters like the mean are typically called **credible intervals**. 

They are called **credible intervals** because they can be interpreted as probabilistic statement about the parameter given the current knowledge and data. In contrast, **confidence intervals** from *likelihood* approaches capture the uncertainty about the interval we have obtained (i.e., whether it contains the true value or not), so they cannot be interpreted as a probabilistic statement about the true parameter values.
