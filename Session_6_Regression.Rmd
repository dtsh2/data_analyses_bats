---
title: "Modelling workshop: Session 6"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r klippy, echo=FALSE, include=TRUE}
library(klippy)
klippy::klippy('')
```


## Session 6

**Session 6** contains an introduction to regression analyses using serology data and model selection techniques.

### 6a - Logistic regression

Earlier we were introduced to *linear regression* in **Session 4**.

In the linear regression we looked at the data were assumed to be from a *normal*, also known as a *Gaussian*, distribution. This is the familiar probability distribution that is symmetric and bell-shaped. 

In a normal distribution the majority of observations cluster around the *mean*, with fewer observations further away from the mean. The distribution is characterized by two parameters, the **mean** (**$μ$**) and the **standard deviation** (**$σ$**), which determine the location and spread of the distribution, respectively. The mean is the central point around which the distribution is centered, while the standard deviation measures how much the observations deviate from the mean. We will use different distributions later and we used some different ones earlier to simulate data.

However, *serological data* are usually converted to *antibody (sero-)* **positive** or *antibody (sero-)* **negative** and so there is a **single binary dependent variable**. 

**Logistic regression** is a statistical technique that we use to model the relationship between a binary dependent variable and one or more independent variables. 

Here our dependent variable can take on only two values, positive or negative. In logistic regression, the model predicts the **probability** of the dependent variable taking on a specific value, given the values of the independent variables. 

The output of logistic regression is a set of coefficients, which can be used to estimate the probability of the dependent variable given a set of values for the independent variables.

Let's import our recently created and saved data set, **example_data.csv** and begin:

```{r, class.source='klippy'}
library(tidyverse)
example_data <- read.csv("data/example_data.csv")
glimpse(example_data)
```

Let's change the dates and factors again:

```{r, class.source='klippy'}
example_data <- example_data %>%
  mutate_at(vars("Cohort", "Sex", "Result.Beckman", "Result.Roche"), as.factor)
example_data <- example_data %>%
  mutate_at(vars("Date.of.birth","Date.of.symptoms","Date.of.PCR.test","Date.of.blood.sample"), ~ as.Date(., format = "%Y-%m-%d"))

glimpse(example_data)
```


Now let's think about the *binary antibody test results* data; let's choose *Result.Beckman*:

```{r, class.source='klippy'}
print(example_data$Result.Beckman)
```

This is binary data. Now lets plot that in the *base* plot:

```{r, class.source='klippy'}
plot(example_data$Result.Beckman)
```

We can see that not all the data are *NEG* or *POS*, so we will have to exclude those for analyses. One simple way is to make a new column using a conditional statement. 

```{r, class.source='klippy'}
library(tidyverse)

# use mutate() to create new column based on factors in column x
example_data <- example_data %>%
  mutate(Test.result = case_when(
    Result.Beckman == "POS" ~ 1,
    Result.Beckman == "NEG" ~ 0,
    Result.Beckman == "-" ~ NA
  ))

# view updated data frame
glimpse(example_data)

```

Let's look at the probability of an antibody test result (*Test.result*; the **response variable**) to one **predictor variable**; here we will use *age*. We will use the **gml()** function and specify **family = binomial** to use the logistic model.

```{r, class.source='klippy'}
model1 <- glm(Test.result ~ Age, example_data, family = binomial)
```

We can look at the model in different ways:

```{r, class.source='klippy'}
model1
summary(model1)
```

In both ways of viewing the results we can see some key results. These are:
* the model itself (after **Call**); 
* the **Coefficients** table that lists the estimated regression coefficients, and 
* the **Summary** provides more details, such as the error (**Std. Error**) and p-values (**Pr(>|z|)**) for each predictor variable in the model. 

The *coefficients* represent the **change in the log odds** of the outcome variable associated with a one-unit increase in the predictor variable, holding all other variables constant.

The **p-value** associated with each coefficient indicates the probability of observing such an extreme result (or more extreme) if the null hypothesis (that the coefficient is zero) were true.

The remaining outputs provide information about the model's goodness of fit, such as the **deviance**, which can be used to assess the overall fit of the model, and the **AIC** that can be used to compare different models and determine the best fit. We will come to this later.

But we can also do some of this straight in ggplot:

```{r, class.source='klippy'}
# Plot Predicted data and original data points
ggplot(example_data, aes(x=Age, y=Test.result)) + 
  geom_point() +
      stat_smooth(method="glm", color="blue", se=FALSE,
                method.args = list(family=binomial))
```

This suggests there is not a relationship with age and the test result, as the line looks very straight horizontally. This is seen in the *glm* results above, where the coefficient is close to zero. And we can add **confidence intervals** around the fitted line to see how much **uncertainty** there is around that.

```{r, class.source='klippy'}
# Plot Predicted data and original data points
ggplot(example_data, aes(x=Age, y=Test.result)) + 
  geom_point() +
      stat_smooth(method="glm", color="blue", se=T,
                method.args = list(family=binomial))
```

Now we can can add another predictor variable and by using some other packages plot the results a little differently:

```{r, class.source='klippy'}
# Load necessary packages
library(ggplot2)
library(dplyr)
library(broom)
library(ggiraphExtra)

model2 = glm(Test.result ~ Age * Sex, example_data, family = binomial)
summary(model2)
ggPredict(model2, se=TRUE, interactive=F,digits=3)

```

Note that nothing here is significant in our model, partly because our sample is really all of positive cases. But let's test the hypothesis that the time since the onset of symptoms predicts the test result. 

```{r, class.source='klippy'}
# Load necessary packages
library(ggplot2)
library(dplyr)
library(broom)
library(ggiraphExtra)

model3 = glm(Test.result ~ Time.from.symptoms.to.blood.sample, example_data, family = binomial)
summary(model3)
ggPredict(model3, se=TRUE, interactive=F,digits=3)

```

Time from symptoms to the blood sample is a better predictor of the test result than age and sex, and the p-value (0.03) is below 0.05, a commonly used cut off for *significance*.

Now let's **simulate** a larger data set that has some more important and significant predictor variables that have a greater effect on the response and analyse this data set. We will simulate a data set with age, sex and location, all things we might be interested in predicting infection in bat populations:

```{r, class.source='klippy'}
# Set random seed for reproducibility
set.seed(123)

# Simulate age data (random integers between 18 and 65)
age <- sample(18:65, 1000, replace = TRUE)

# Simulate sex data (random binary values)
sex <- sample(c(0,1), 1000, replace = TRUE)

# Simulate location data (random values from four categories)
location <- sample(c("Location1", "Location2", "Location3", "Location4"), 1000, replace = TRUE)

# Simulate binomial response variable predicted by age and location but not sex
prob <- exp(0.02*age + ifelse(location=="Location1", 0.5,
                              ifelse(location=="Location2", -0.5,
                                     ifelse(location=="Location3", 1, 0)))) / (1 + exp(0.02*age + ifelse(location=="Location1", 0.5,
                              ifelse(location=="Location2", -0.5,
                                     ifelse(location=="Location3", 1, 0)))))

response <- rbinom(1000, 1, prob)
df<-data.frame(response = response,
      location = location,
      age = age,
      sex = sex)
```

Now let's try the logistic model:

```{r, class.source='klippy'}
library(ggplot2)
library(ggiraphExtra)

model4 = glm(response ~ location + age + sex, df, family = binomial)
summary(model4)
#ggPredict(model4, interactive = T, se=T)

```
Note the significant coavariates above. An easy way to see them is with the **$ * $** or **$ ** $**. But note effect sizes are also important. 

Now let's look at some interactions by replacing the **+** in the above equations with **$*$**:

```{r, class.source='klippy'}
library(ggplot2)
library(ggiraphExtra)

model5 = glm(response ~ location * age * sex, df, family = binomial)
summary(model5)
```

This is the type of model that you might use if you think that every location and both sexes (here binary for bats) had different results that varied with age. This is quite complex and while interactions can be extremely important, these analyses need to be undertaken with care.

### 6b - Model selection

##### Choose a model by AIC in a Stepwise Algorithm

The above model was complex and allowed all interactions between location, age and sex in our simulated data set. However, we can use an approach that uses the **AIC**, which we introduced earlier, to allow us to work through the models and simplify them to choose the **best** model, given our data.

AIC is a statistical measure used for model selection and comparison and provides a way to balance *model complexity* and *goodness of fit*.

The AIC score is calculated using the following formula: $AIC = -2log(L) + 2k$,

where L represents the maximum likelihood estimation of the model and k is the number of parameters in the model. The **log** is essentially for analytical convenience. The AIC score quantifies the trade-off between model accuracy and complexity.

When comparing multiple models using AIC, the model with the lowest AIC score is considered the best fit. This approach considers both the goodness of fit (captured by the likelihood term, L) and the model complexity (represented by the number of parameters, k). By penalizing models with a higher number of parameters, AIC helps to prevent **overfitting** and encourages the selection of simpler models.

Here's what that looks like using the **stepAIC()** function in the **MASS** package.

```{r, class.source='klippy'}
library(MASS)
library(tidyverse)
# Fit the model
model <- glm(response ~., data = df, family = binomial) %>%
  stepAIC(trace = FALSE)
# Summarize the final selected model
summary(model)
# Make predictions
probabilities <- model %>% predict(df, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
# Model accuracy
mean(predicted.classes==df$response)
```
If you change ``trace = FALSE'' to ``trace = TRUE'' you will see the steps.

##### Plotting regression models

There are many plots that can be done after doing regression analyses, including of the residuals, but for now we will simply plot the model coefficient results.

```{r, class.source='klippy'}
# Load required libraries
library(ggplot2)
library(GGally)
# Create a dummy dataset
ggcoef(model)

```
Here we can see the point estimates, their 95\% confidence intervals and if these are greater than or less than 0.
